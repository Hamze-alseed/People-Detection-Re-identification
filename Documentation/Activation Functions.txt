ReLU:

ReLU(x)=max(0,x)

 الخصائص : 
 الالخطية: بالرغم من أنها تبدو بسيطة وخطية, الا أن ReLU تدخل الاخطية في النموذج مما يسمح له بتعلم الأنماط المعقدة

 التناثر: يمكن أن تكون مخرجات ReLU صفرا ألي مدخل أقل من الصفر، مما يؤدي إلى التناثر في التفعيل. وهذا يعني أن مجموعة فرعية فقط من الخاليا 
العصبية يتم تفعيلها في أي وقت، مما يمكن أن يحسن كفاءة النموذج.

الكفاءة الحسابية: فعال حسابيا  لانه يتضمن فقط عملية العتبة عند الصفر، وهي أقل تكلفة من الدوال الاكثر 
ً
تعقيد مثل 

sigmoid/tanh


 المزايا : 
• تخفيف مشكلة تلاشي التدرجات : على عكس السجمويد والتانجنت الهيبريدي 
 ، لا تعاني ReLU من مشكلة تلاشي التدرجات لان تدرجها 
إما 0 أو 1 لمعظم المدخلات، مما يمنع التدرجات من أن تصبح صغيرة ا
جدا 
أثناء الانتشار العكسي. 
• التنفيذ البسيط: دالة  سهلة التنفيذ وسريعة الحساب .
• تحسين التقارب : تجريبيا النماذج التي تستخدم تفعيل ReLU تميل إلى التقارب
بشكل أسرع من تلك التي تستخدم تفعيل السجمويد /التانجنت 


 العيوب :
• مشكلة الخلايا العصبية الميتة: يمكن لبعض الخلايا العصبية أن تتوقف أثناء 
التدريب وتخرج دائما صفر إذا كانت في الجانب السلبي من ReLU. تم معالجة 
هذه المشكلة من خلال متغيرات مثل ReLU Leaky و ReLU Parametric و
 .Exponential Linear Units (ELUs)
• المخرجات غير المقيدة: يمكن أن تنتج ReLU قيم كبيرة جدا مما قد يتطلب 
التهيئة الدقيقة وتطبيع البيانات المدخلة